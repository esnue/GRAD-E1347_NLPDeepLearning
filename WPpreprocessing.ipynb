{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WPpreprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNo/tRGWu+EF/lWpGs59x1V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esnue/ThesisAllocationSystem/blob/main/WPpreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygKSIG-pbPMs",
        "outputId": "fc8c46e6-3768-4744-837f-4b1cab0257bc"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import string\n",
        "from nltk.corpus import stopwords as stop_words\n",
        "import warnings\n",
        "\n",
        "stoplist = \"\"\"gov anheier xxx et enrichsource al rgreq al often nin nm nd nw nn ns ng np ne na nat nt nio may net even use publicationcoverpdf \\\\n \\n researchgate \n",
        "likely dataset nwe aims throughout nbecause nonly gov ncan ncan even bryson simple reducing net example jankin results first esc shows also utc jstor nu nh nb doi \n",
        "www tandfonline nhttps nof nthe well new nand paper co however kreyenfeld \\\\xa0See \\\\xa0BIaS? wegrich mena nsource ny el ng nri nio neu nbut nif ets echr used\"\"\"\n",
        "\n",
        "class WhiteSpacePreprocessing():\n",
        "\n",
        "    def __init__(self, documents, stopwords_language=\"english\", vocabulary_size=2000):\n",
        "\n",
        "        stop_words.extend(stoplist)\n",
        "        self.documents = documents\n",
        "        self.stopwords = set(stop_words.words(stopwords_language))\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "\n",
        "    def preprocess(self):\n",
        "\n",
        "        preprocessed_docs_tmp = self.documents\n",
        "        preprocessed_docs_tmp = [doc.lower() for doc in preprocessed_docs_tmp]\n",
        "        preprocessed_docs_tmp = [doc.translate(\n",
        "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
        "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords])\n",
        "                             for doc in preprocessed_docs_tmp]\n",
        "\n",
        "        vectorizer = CountVectorizer(max_features=self.vocabulary_size, token_pattern=r'\\b[a-zA-Z]{2,}\\b')\n",
        "        vectorizer.fit_transform(preprocessed_docs_tmp)\n",
        "        vocabulary = set(vectorizer.get_feature_names())\n",
        "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w in vocabulary])\n",
        "                                 for doc in preprocessed_docs_tmp]\n",
        "\n",
        "        preprocessed_docs, unpreprocessed_docs = [], []\n",
        "        for i, doc in enumerate(preprocessed_docs_tmp):\n",
        "            if len(doc) > 0:\n",
        "                preprocessed_docs.append(doc)\n",
        "                unpreprocessed_docs.append(self.documents[i])\n",
        "\n",
        "        return preprocessed_docs, unpreprocessed_docs, list(vocabulary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<WordListCorpusReader in '.../corpora/stopwords' (not loaded yet)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    }
  ]
}